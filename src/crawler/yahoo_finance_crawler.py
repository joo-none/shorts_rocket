# -*- coding: utf-8 -*-
"""
yahoo_finance_crawler.py - Yahoo Finance ë‰´ìŠ¤ í¬ë¡¤ëŸ¬

Playwright ê¸°ë°˜ + ë´‡ ê°ì§€ ìš°íšŒ
"""

import subprocess
import sys
import platform
import os
import io
import requests
from pathlib import Path

# Windows ì½˜ì†” UTF-8 ì„¤ì •
if platform.system() == "Windows":
    try:
        subprocess.run("chcp 65001", shell=True, capture_output=True, check=True)
    except:
        pass

os.environ["PYTHONIOENCODING"] = "utf-8"
os.environ["PYTHONLEGACYWINDOWSSTDIO"] = "utf-8"

try:
    if hasattr(sys.stdout, "buffer"):
        sys.stdout = io.TextIOWrapper(
            sys.stdout.buffer, encoding="utf-8", errors="replace", line_buffering=True
        )
    if hasattr(sys.stderr, "buffer"):
        sys.stderr = io.TextIOWrapper(
            sys.stderr.buffer, encoding="utf-8", errors="replace", line_buffering=True
        )
except:
    pass

from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout
from bs4 import BeautifulSoup
import time
import random
import logging
import re
from urllib.parse import urljoin, urlparse

# ë¡œê¹… ì„¤ì •
logging.getLogger().handlers.clear()
logger = logging.getLogger()
logger.setLevel(logging.INFO)

console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# ì„¤ì •
MAX_RETRIES = 5
MIN_DELAY = 2
MAX_DELAY = 4
BASE_URL = "https://finance.yahoo.com/quote/"

# Lambda í™˜ê²½ ê°ì§€
IS_LAMBDA = os.environ.get("AWS_LAMBDA_FUNCTION_NAME") is not None
TEMP_DIR = "/tmp" if IS_LAMBDA else "temp_images"

if not IS_LAMBDA:
    Path(TEMP_DIR).mkdir(exist_ok=True)
    logger.info(f"ì„ì‹œ ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ: {os.path.abspath(TEMP_DIR)}")


class YahooFinanceCrawler:
    def __init__(self):
        self.playwright = None
        self.browser = None
        self.context = None

    def parse_time_ago(self, time_str):
        """ì‹œê°„ ë¬¸ìì—´ì„ ë¶„ ë‹¨ìœ„ë¡œ ë³€í™˜ (ì˜ˆ: '41m ago' â†’ 41, '2h ago' â†’ 120, '1d ago' â†’ 1440)"""
        try:
            # ìˆ«ìì™€ ë‹¨ìœ„ ì¶”ì¶œ
            match = re.search(r"(\d+)\s*([mhd])", time_str.lower())
            if not match:
                return float("inf")  # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê°€ì¥ ì˜¤ë˜ëœ ê²ƒìœ¼ë¡œ ì²˜ë¦¬

            value = int(match.group(1))
            unit = match.group(2)

            if unit == "m":  # minutes
                return value
            elif unit == "h":  # hours
                return value * 60
            elif unit == "d":  # days
                return value * 60 * 24
            else:
                return float("inf")
        except Exception as e:
            logger.debug(f"ì‹œê°„ íŒŒì‹± ì˜¤ë¥˜: {time_str} - {e}")
            return float("inf")

    def __enter__(self):
        # Stealth ëª¨ë“ˆ ì²´í¬
        try:
            from playwright_stealth import stealth_sync

            self.use_stealth = True
            logger.info("âœ“ playwright-stealth ë¡œë“œ ì„±ê³µ")
        except ImportError:
            self.use_stealth = False
            logger.warning("âš  playwright-stealth ì—†ìŒ. ê¸°ë³¸ ìš°íšŒë§Œ ì‚¬ìš©")

        self.playwright = sync_playwright().start()

        # ë¸Œë¼ìš°ì € ì‹¤í–‰
        self.browser = self.playwright.chromium.launch(
            headless=False,  # ë””ë²„ê¹…ìš© (ë°°í¬ì‹œ Trueë¡œ ë³€ê²½)
            args=[
                "--disable-blink-features=AutomationControlled",
                "--disable-dev-shm-usage",
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-web-security",
                "--disable-features=IsolateOrigins,site-per-process",
            ],
        )

        # ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
        self.context = self.browser.new_context(
            viewport={"width": 1920, "height": 1080},
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            locale="en-US",
            extra_http_headers={
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
            },
        )

        # Stealth ëª¨ë“œ ì ìš©
        if self.use_stealth:
            from playwright_stealth import stealth_sync

            page = self.context.new_page()
            stealth_sync(page)
            page.close()
            logger.info("âœ“ Stealth ëª¨ë“œ í™œì„±í™”")

        # ë´‡ ê°ì§€ ìš°íšŒ ìŠ¤í¬ë¦½íŠ¸
        self.context.add_init_script(
            """
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
            
            window.chrome = {
                runtime: {},
                loadTimes: function() {},
                csi: function() {},
                app: {}
            };
            
            const originalQuery = window.navigator.permissions.query;
            window.navigator.permissions.query = (parameters) => (
                parameters.name === 'notifications' ?
                    Promise.resolve({ state: Notification.permission }) :
                    originalQuery(parameters)
            );
            
            Object.defineProperties(navigator, {
                plugins: { get: () => [1, 2, 3, 4, 5] },
                languages: { get: () => ['en-US', 'en'] },
                platform: { get: () => 'Win32' },
                hardwareConcurrency: { get: () => 8 },
                deviceMemory: { get: () => 8 }
            });
        """
        )

        logger.info("âœ“ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.context:
            self.context.close()
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()

    def close_popup_if_exists(self, page):
        """Yahoo Finance íŒì—…/ë°°ë„ˆ ë‹«ê¸°"""
        try:
            # ì¿ í‚¤ ë™ì˜ ë°°ë„ˆ
            cookie_selectors = [
                'button[name="agree"]',
                '[data-testid="consent-accept-all"]',
                ".consent-overlay button.accept",
                'button:has-text("Accept all")',
                'button:has-text("Accept")',
            ]

            for selector in cookie_selectors:
                try:
                    if page.locator(selector).count() > 0:
                        page.click(selector, timeout=3000)
                        logger.info("âœ“ ì¿ í‚¤ ë™ì˜ í´ë¦­")
                        time.sleep(1)
                        return True
                except:
                    continue

            # ì¼ë°˜ ëª¨ë‹¬ ë‹«ê¸°
            close_selectors = [
                'button[aria-label="Close"]',
                'button[aria-label*="close" i]',
                '[data-testid="close-button"]',
                ".modal-close",
                "button.close",
            ]

            for selector in close_selectors:
                try:
                    if page.locator(selector).count() > 0:
                        page.click(selector, timeout=2000)
                        logger.info("âœ“ ëª¨ë‹¬ ë‹«ê¸° í´ë¦­")
                        time.sleep(1)
                        return True
                except:
                    continue

            # ESC í‚¤
            try:
                page.keyboard.press("Escape")
                time.sleep(0.5)
            except:
                pass

            return False

        except Exception as e:
            logger.debug(f"íŒì—… ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return False

    def simulate_human_behavior(self, page):
        """ì‚¬ëŒì²˜ëŸ¼ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜"""
        try:
            time.sleep(random.uniform(1.5, 3))

            # ë§ˆìš°ìŠ¤ ì´ë™
            page.mouse.move(random.randint(100, 500), random.randint(100, 500))
            time.sleep(random.uniform(0.3, 0.8))

            # ìŠ¤í¬ë¡¤ (ë‰´ìŠ¤ ë” ë¡œë“œí•˜ê¸° ìœ„í•´)
            for _ in range(random.randint(2, 4)):
                scroll_amount = random.randint(300, 600)
                page.evaluate(f"window.scrollBy(0, {scroll_amount})")
                time.sleep(random.uniform(0.5, 1.2))

            # ë§¨ ìœ„ë¡œ
            page.evaluate("window.scrollTo(0, 0)")
            time.sleep(random.uniform(0.5, 1.0))

        except Exception as e:
            logger.debug(f"í–‰ë™ ì‹œë®¬ë ˆì´ì…˜ ì˜¤ë¥˜: {e}")

    def download_image(self, image_url, article_url, ticker, image_index=1):
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        try:
            # íŒŒì¼ëª… ìƒì„±
            parsed = urlparse(article_url)
            article_id = parsed.path.split("/")[-1][:30] or f"article_{image_index}"

            ext = image_url.split(".")[-1].split("?")[0]
            if ext not in ["jpg", "jpeg", "png", "gif", "webp"]:
                ext = "jpg"

            filename = f"{ticker}_{article_id}_{image_index}.{ext}"
            filepath = os.path.join(TEMP_DIR, filename)

            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Referer": "https://finance.yahoo.com/",
            }

            response = requests.get(image_url, headers=headers, timeout=10)
            if response.status_code == 200:
                with open(filepath, "wb") as f:
                    f.write(response.content)
                logger.info(f"âœ“ ì´ë¯¸ì§€ ì €ì¥: {filename}")
                return filepath
            return None

        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None

    def extract_images(self, soup, article_url, ticker):
        """ê¸°ì‚¬ ì´ë¯¸ì§€ ì¶”ì¶œ"""
        images = []
        image_counter = 1
        seen_urls = set()

        # ë©”ì¸ ì´ë¯¸ì§€
        main_selectors = [
            'img[data-testid="lead-image"]',
            "article img",
            ".caas-img-container img",
            ".caas-body img",
        ]

        for selector in main_selectors:
            for img in soup.select(selector):
                image_url = img.get("src") or img.get("data-src")
                if not image_url or image_url in seen_urls:
                    continue

                # ì‘ì€ ì•„ì´ì½˜ ì œì™¸
                if "icon" in image_url.lower() or "logo" in image_url.lower():
                    continue
                if "avatar" in image_url.lower():
                    continue

                seen_urls.add(image_url)

                if image_url.startswith("//"):
                    image_url = "https:" + image_url
                elif not image_url.startswith("http"):
                    image_url = urljoin("https://finance.yahoo.com", image_url)

                filepath = self.download_image(
                    image_url, article_url, ticker, image_counter
                )
                if filepath:
                    images.append(
                        {
                            "url": image_url,
                            "filepath": filepath,
                            "alt": img.get("alt", ""),
                            "type": "main" if image_counter == 1 else "content",
                        }
                    )
                    image_counter += 1

        return images

    def fetch_url(self, page, url):
        """URL ê°€ì ¸ì˜¤ê¸°"""
        for attempt in range(1, MAX_RETRIES + 1):
            try:
                logger.info(f"ğŸ“„ [{attempt}/{MAX_RETRIES}] ë¡œë”©: {url}")

                if attempt > 1:
                    wait_time = random.uniform(5, 10) * attempt
                    logger.info(f"â³ {wait_time:.1f}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(wait_time)

                # í˜ì´ì§€ ë¡œë“œ
                response = page.goto(url, wait_until="domcontentloaded", timeout=60000)

                if response:
                    logger.info(f"ìƒíƒœ ì½”ë“œ: {response.status}")

                # ë¡œë”© ëŒ€ê¸°
                time.sleep(random.uniform(3, 5))

                # íŒì—… ì²˜ë¦¬
                self.close_popup_if_exists(page)

                # ì‚¬ëŒì²˜ëŸ¼ í–‰ë™
                self.simulate_human_behavior(page)

                html = page.content()

                # ê²€ì¦
                if len(html) < 3000:
                    logger.warning(f"âš  HTML ë„ˆë¬´ ì§§ìŒ: {len(html)} bytes")
                    time.sleep(5 * attempt)
                    continue

                logger.info(f"âœ“ ì„±ê³µ ({len(html):,} bytes)")
                return html

            except PlaywrightTimeout:
                logger.error(f"â± íƒ€ì„ì•„ì›ƒ ({attempt}/{MAX_RETRIES})")
                time.sleep(10 * attempt)
            except Exception as e:
                logger.error(f"âŒ ì˜¤ë¥˜ ({attempt}/{MAX_RETRIES}): {e}")
                time.sleep(8 * attempt)

        logger.error(f"âŒ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨: {url}")
        return None

    def extract_article_body(self, soup):
        """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        # Yahoo Finance ê¸°ì‚¬ ë³¸ë¬¸ ì…€ë ‰í„°
        body_selectors = [
            ".caas-body",
            '[data-testid="article-body"]',
            "article .body",
            ".article-body",
        ]

        for selector in body_selectors:
            body_elem = soup.select_one(selector)
            if body_elem:
                # ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ ì œê±°
                for tag in body_elem(["script", "style", "aside", "nav"]):
                    tag.decompose()
                text = body_elem.get_text(separator="\n", strip=True)
                if len(text) > 100:
                    return text

        # fallback: ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ë¸”ë¡ ì°¾ê¸°
        candidates = soup.find_all(["article", "main", "div"])
        max_text = ""
        for c in candidates:
            for tag in c(["script", "style"]):
                tag.decompose()
            text = c.get_text(separator="\n", strip=True)
            if len(text) > len(max_text):
                max_text = text
        return max_text

    def extract_news_links(self, soup, ticker, max_articles=10):
        """ë‰´ìŠ¤ ëª©ë¡ì—ì„œ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ (ìµœì‹ ìˆœ ì •ë ¬)"""
        news_links = []
        seen_urls = set()

        # ê¸°ì‚¬ ì»¨í…Œì´ë„ˆ ì°¾ê¸°: li.stream-item.story-item
        article_containers = soup.select("li.stream-item.story-item")
        logger.info(f"ğŸ” ê¸°ì‚¬ ì»¨í…Œì´ë„ˆ {len(article_containers)}ê°œ ë°œê²¬")

        for container in article_containers:
            # ë§í¬ ì°¾ê¸° (titles í´ë˜ìŠ¤ê°€ ìˆëŠ” a íƒœê·¸)
            a_tag = container.select_one('a.titles, a[class*="titles"]')
            if not a_tag:
                continue

            href = a_tag.get("href", "")

            # ì œëª© ì°¾ê¸° (h3.clamp)
            h3_tag = a_tag.select_one("h3.clamp")
            title = (
                h3_tag.get_text(strip=True) if h3_tag else a_tag.get_text(strip=True)
            )

            if not href or not title:
                continue

            # ì „ì²´ URL ìƒì„±
            if href.startswith("/"):
                full_url = f"https://finance.yahoo.com{href}"
            elif not href.startswith("http"):
                full_url = urljoin("https://finance.yahoo.com", href)
            else:
                full_url = href

            # ì¤‘ë³µ ì²´í¬
            if full_url in seen_urls:
                continue

            # ë¹„ë””ì˜¤/ê´‘ê³  ì œì™¸
            if "/video/" in full_url:
                continue
            if "ad.doubleclick" in full_url:
                continue

            # ì‹œê°„ ì •ë³´ ì¶”ì¶œ (.publishing)
            time_str = ""
            time_minutes = float("inf")

            time_elem = container.select_one('.publishing, div[class*="publishing"]')
            if time_elem:
                time_text = time_elem.get_text(strip=True)
                # "Reuters Videos â€¢ 36m ago" ì—ì„œ ì‹œê°„ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                time_match = re.search(r"(\d+)\s*([mhd])\s*ago", time_text.lower())
                if time_match:
                    time_str = f"{time_match.group(1)}{time_match.group(2)} ago"
                    time_minutes = self.parse_time_ago(time_str)

            # ì‹œê°„ ì •ë³´ ì—†ìœ¼ë©´ ì œì™¸
            if not time_str:
                logger.debug(f"ì‹œê°„ ì •ë³´ ì—†ìŒ, ì œì™¸: {title[:40]}...")
                continue

            seen_urls.add(full_url)
            news_links.append(
                {
                    "title": title,
                    "url": full_url,
                    "time_str": time_str,
                    "time_minutes": time_minutes,
                }
            )

        # ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ìµœì‹ ìˆœ = ë¶„ì´ ì‘ì€ ìˆœ)
        news_links.sort(key=lambda x: x["time_minutes"])

        # ìƒìœ„ max_articlesê°œë§Œ ì„ íƒ
        news_links = news_links[:max_articles]

        # ë¡œê·¸ ì¶œë ¥
        for idx, news in enumerate(news_links, start=1):
            logger.info(f"[{idx:2d}] ğŸ“ {news['title']}")
            logger.info(f"      ğŸ”— {news['url']}")
            logger.info(f"      â° {news['time_str']}")

        logger.info(f"\nğŸ“° ìµœì‹ ìˆœ ì •ë ¬ í›„ ìƒìœ„ {len(news_links)}ê°œ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ")
        return news_links

    def crawl_company(self, ticker, count):
        """íšŒì‚¬(í‹°ì»¤) ë‰´ìŠ¤ ìˆ˜ì§‘"""
        results = []
        seen_titles = set()

        news_page_url = f"{BASE_URL}{ticker}/news/"
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ” {ticker} í¬ë¡¤ë§ ì‹œì‘")
        logger.info(f"ğŸ”— {news_page_url}")
        logger.info(f"{'='*70}\n")

        page = self.context.new_page()

        try:
            # ë‰´ìŠ¤ ëª©ë¡ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            html = self.fetch_url(page, news_page_url)
            if html is None:
                return results

            soup = BeautifulSoup(html, "lxml")

            # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
            news_links = self.extract_news_links(soup, ticker, max_articles=10)

            logger.info("=" * 70)

            if not news_links:
                logger.warning(f"âš  ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨. HTML êµ¬ì¡° í™•ì¸ í•„ìš”")
                # ë””ë²„ê¹…ìš© HTML ì €ì¥
                debug_path = os.path.join(TEMP_DIR, f"{ticker}_debug.html")
                with open(debug_path, "w", encoding="utf-8") as f:
                    f.write(html)
                logger.info(f"ë””ë²„ê¹…ìš© HTML ì €ì¥: {debug_path}")
                return results

            # ê° ê¸°ì‚¬ í¬ë¡¤ë§
            for idx, news_item in enumerate(news_links[:count], start=1):
                article_url = news_item["url"]

                logger.info(f"\n{'='*70}")
                logger.info(f"ğŸ“„ ê¸°ì‚¬ [{idx}/{count}] í¬ë¡¤ë§ ì¤‘...")
                logger.info(f"ğŸ”— {article_url}")
                logger.info(f"{'='*70}")

                # ê¸°ì‚¬ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
                article_html = self.fetch_url(page, article_url)
                if article_html is None:
                    logger.error(f"âŒ ê¸°ì‚¬ [{idx}] ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")
                    continue

                article_soup = BeautifulSoup(article_html, "lxml")

                # ì œëª© ì¶”ì¶œ (Yahoo Finance cover-headline ìš°ì„ )
                title_selectors = [
                    ".cover-headline h1.cover-title",  # Yahoo Finance ë©”ì¸ ì œëª©
                    "h1.cover-title",
                    'h1[data-testid="article-title"]',
                    "header h1",
                    "article h1",
                    "h1",
                ]

                title = news_item["title"]
                for selector in title_selectors:
                    title_tag = article_soup.select_one(selector)
                    if title_tag:
                        title = title_tag.get_text(strip=True)
                        break

                # ì¤‘ë³µ ì œëª© ì²´í¬
                if title in seen_titles:
                    logger.warning(f"âš  ì¤‘ë³µ ì œëª©: {title}")
                    continue
                seen_titles.add(title)

                # ë³¸ë¬¸ ì¶”ì¶œ
                body = self.extract_article_body(article_soup)

                # ì´ë¯¸ì§€ ì¶”ì¶œ
                images = self.extract_images(article_soup, article_url, ticker)

                logger.info(f"âœ… ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ!")
                logger.info(f"   ğŸ“Œ ì œëª©: {title}")
                logger.info(f"   ğŸ”— ë§í¬: {article_url}")
                logger.info(f"   â° ê²Œì‹œ: {news_item.get('time_str', 'ì‹œê°„ ì—†ìŒ')}")
                logger.info(
                    f"   ğŸ“ ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {body[:100].replace(chr(10), ' ')}..."
                )
                logger.info(f"   ğŸ“ ë³¸ë¬¸ ê¸¸ì´: {len(body):,}ì")
                logger.info(f"   ğŸ–¼ï¸  ì´ë¯¸ì§€: {len(images)}ê°œ")

                results.append(
                    {
                        "ticker": ticker,
                        "title": title,
                        "body": body,
                        "url": article_url,
                        "images": images,
                        "time_ago": news_item.get("time_str", ""),
                    }
                )

                # ìš”ì²­ ê°„ ë”œë ˆì´
                time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))

        finally:
            page.close()

        logger.info(f"\n{'='*70}")
        logger.info(f"âœ… {ticker} í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(results)}ê°œ ê¸°ì‚¬")
        logger.info(f"{'='*70}\n")

        return results


def crawl_all(company_list):
    """ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰"""
    # ì…ë ¥ í˜•ì‹ í‘œì¤€í™”
    standardized_list = []
    for item in company_list:
        if isinstance(item, dict):
            standardized_list.append(item)
        elif isinstance(item, (list, tuple)) and len(item) == 2:
            standardized_list.append({"name": item[0], "count": item[1]})

    all_results = []

    with YahooFinanceCrawler() as crawler:
        for comp in standardized_list:
            ticker = comp["name"]
            count = comp["count"]
            comp_results = crawler.crawl_company(ticker, count)
            all_results.extend(comp_results)

    return all_results
